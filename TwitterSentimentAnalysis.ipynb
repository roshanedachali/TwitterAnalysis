{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitterSentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WfztzZVFDYQIrny_fxcvl91xVDy2MX0P",
      "authorship_tag": "ABX9TyMLvOumumyYWYQo913HpuSX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roshanedachali/TwitterAnalysis/blob/main/TwitterSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzx6_jKrwFej",
        "outputId": "a4695402-f9b3-4d5b-d223-5db9cd746a5b"
      },
      "source": [
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import ast\n",
        "import yaml\n",
        "def create_twitter_url():\n",
        "    handle = \"roshanedachali_\" #Add handle here\n",
        "    max_results = 100 #Measures number of tweets studied \n",
        "    mrf = \"max_results={}\".format(max_results)\n",
        "    q = \"query=from:{}\".format(handle)\n",
        "    url = \"https://api.twitter.com/2/tweets/search/recent?{}&{}\".format(\n",
        "        mrf, q\n",
        "    )\n",
        "    return url #Twitter API\n",
        "def create_bearer_token(data):\n",
        "    return data[\"search_tweets_api\"][\"bearer_token\"] #Developer account tokens\n",
        "def twitter_auth_and_connect(bearer_token, url): \n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "    response = requests.request(\"GET\", url, headers=headers)\n",
        "    return response.json()\n",
        "def lang_data_shape(res_json):\n",
        "    data_only = res_json[\"data\"]\n",
        "    doc_start = '\"documents\": {}'.format(data_only)\n",
        "    str_json = \"{\" + doc_start + \"}\"\n",
        "    dump_doc = json.dumps(str_json)\n",
        "    doc = json.loads(dump_doc)\n",
        "    return ast.literal_eval(doc)\n",
        "def connect_to_azure(data):\n",
        "    azure_url = \"https://week.cognitiveservices.azure.com/\"\n",
        "    language_api_url = \"{}text/analytics/v2.1/languages\".format(azure_url)\n",
        "    sentiment_url = \"{}text/analytics/v2.1/sentiment\".format(azure_url)\n",
        "    subscription_key = data[\"azure\"][\"subscription_key\"]\n",
        "    return language_api_url, sentiment_url, subscription_key\n",
        "def generate_languages(headers, language_api_url, documents):\n",
        "    response = requests.post(language_api_url, headers=headers, json=documents)\n",
        "    return response.json()\n",
        "def process_yaml():\n",
        "    with open(\"/config.yaml\") as file: #yaml gets pulled from drive\n",
        "        return yaml.safe_load(file)\n",
        "def azure_header(subscription_key):\n",
        "    return {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
        "def combine_lang_data(documents, with_languages):\n",
        "    langs = pd.DataFrame(with_languages[\"documents\"])\n",
        "    lang_iso = [x.get(\"iso6391Name\")\n",
        "                for d in langs.detectedLanguages if d for x in d]\n",
        "    data_only = documents[\"documents\"]\n",
        "    tweet_data = pd.DataFrame(data_only)\n",
        "    tweet_data.insert(2, \"language\", lang_iso, True)\n",
        "    json_lines = tweet_data.to_json(orient=\"records\")\n",
        "    return json_lines\n",
        "def add_document_format(json_lines):\n",
        "    docu_format = '\"' + \"documents\" + '\"'\n",
        "    json_docu_format = \"{}:{}\".format(docu_format, json_lines)\n",
        "    docu_align = \"{\" + json_docu_format + \"}\"\n",
        "    jd_align = json.dumps(docu_align)\n",
        "    jl_align = json.loads(jd_align)\n",
        "    return ast.literal_eval(jl_align)\n",
        "def sentiment_scores(headers, sentiment_url, document_format):\n",
        "    response = requests.post(\n",
        "        sentiment_url, headers=headers, json=document_format)\n",
        "    return response.json()\n",
        "def week_logic(week_score):\n",
        "    if week_score > 0.75 or week_score == 0.75:\n",
        "        print(\"You had a positive week\")\n",
        "    elif week_score > 0.45 or week_score == 0.45:\n",
        "        print(\"You had a neutral week\")\n",
        "    else:\n",
        "        print(\"You had a negative week, I hope it gets better\")\n",
        "def mean_score(sentiments):\n",
        "    sentiment_df = pd.DataFrame(sentiments[\"documents\"])\n",
        "    return sentiment_df[\"score\"].mean()\n",
        "\n",
        "#Replacement for main\n",
        "url = create_twitter_url()\n",
        "data = process_yaml()\n",
        "bearer_token = create_bearer_token(data)\n",
        "res_json = twitter_auth_and_connect(bearer_token, url)\n",
        "documents = lang_data_shape(res_json)\n",
        "language_api_url, sentiment_url, subscription_key = connect_to_azure(data)\n",
        "headers = azure_header(subscription_key)\n",
        "with_languages = generate_languages(headers, language_api_url, documents)\n",
        "json_lines = combine_lang_data(documents, with_languages)\n",
        "document_format = add_document_format(json_lines)\n",
        "sentiments = sentiment_scores(headers, sentiment_url, document_format)\n",
        "week_score = mean_score(sentiments)\n",
        "print(week_score)\n",
        "week_logic(week_score)\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.44394260048866274\n",
            "You had a negative week, I hope it gets better\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXD9LvlZKD61"
      },
      "source": [
        ""
      ]
    }
  ]
}